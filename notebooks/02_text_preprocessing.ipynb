{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "353effda",
   "metadata": {},
   "source": [
    "# üßπ Text Preprocessing & Feature Engineering\n",
    "\n",
    "**Project:** Restaurant Sentiment Analysis  \n",
    "**Author:** Akakinad  \n",
    "**Date:** January 29, 2026  \n",
    "**Objective:** Prepare text data for machine learning by cleaning, tokenizing, and vectorizing reviews\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. Setup & Data Loading\n",
    "2. Text Cleaning\n",
    "3. Tokenization\n",
    "4. TF-IDF Vectorization\n",
    "5. Train-Test Split\n",
    "6. Save Processed Data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fb797b",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup & Data Loading\n",
    "\n",
    "Import libraries and load the cleaned dataset from Phase 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f7aa95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0b35b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ NLTK data downloaded!\n"
     ]
    }
   ],
   "source": [
    "# Download required NLTK data\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "print(\"‚úÖ NLTK data downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "575d0eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset loaded successfully!\n",
      "üìä Shape: (996, 4)\n",
      "\n",
      "First 3 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "      <th>review_length</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Review  Liked  review_length  word_count\n",
       "0                   Wow... Loved this place.      1             24           4\n",
       "1                         Crust is not good.      0             18           4\n",
       "2  Not tasty and the texture was just nasty.      0             41           8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the cleaned dataset from Phase 1\n",
    "df = pd.read_csv('./data/processed/reviews_with_features.csv')\n",
    "\n",
    "print(\"‚úÖ Dataset loaded successfully!\")\n",
    "print(f\"üìä Shape: {df.shape}\")\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed67e48",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Text Cleaning\n",
    "\n",
    "Remove punctuation, numbers, convert to lowercase, and prepare text for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efc574be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Text cleaning function created!\n"
     ]
    }
   ],
   "source": [
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean and normalize text for NLP processing\n",
    "    \n",
    "    Steps:\n",
    "    1. Convert to lowercase\n",
    "    2. Remove punctuation and numbers\n",
    "    3. Remove extra whitespace\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation and numbers, keep only letters and spaces\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "print(\"‚úÖ Text cleaning function created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58016366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE Cleaning:\n",
      "'Wow... Loved this place.'\n",
      "\n",
      "AFTER Cleaning:\n",
      "'wow loved this place'\n"
     ]
    }
   ],
   "source": [
    "# Test the cleaning function\n",
    "sample_review = df['Review'].iloc[0]\n",
    "\n",
    "print(\"BEFORE Cleaning:\")\n",
    "print(f\"'{sample_review}'\")\n",
    "print(\"\\nAFTER Cleaning:\")\n",
    "print(f\"'{clean_text(sample_review)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e6feaa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All reviews cleaned!\n",
      "\n",
      "Dataset now has 5 columns:\n",
      "['Review', 'Liked', 'review_length', 'word_count', 'Review_Clean']\n"
     ]
    }
   ],
   "source": [
    "# Apply cleaning to all reviews\n",
    "df['Review_Clean'] = df['Review'].apply(clean_text)\n",
    "\n",
    "print(\"‚úÖ All reviews cleaned!\")\n",
    "print(f\"\\nDataset now has {len(df.columns)} columns:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0dd165c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before vs After Cleaning:\n",
      "================================================================================\n",
      "\n",
      "1. ORIGINAL: Wow... Loved this place.\n",
      "   CLEANED:  wow loved this place\n",
      "\n",
      "2. ORIGINAL: Crust is not good.\n",
      "   CLEANED:  crust is not good\n",
      "\n",
      "3. ORIGINAL: Not tasty and the texture was just nasty.\n",
      "   CLEANED:  not tasty and the texture was just nasty\n",
      "\n",
      "4. ORIGINAL: Stopped by during the late May bank holiday off Rick Steve recommendation and loved it.\n",
      "   CLEANED:  stopped by during the late may bank holiday off rick steve recommendation and loved it\n",
      "\n",
      "5. ORIGINAL: The selection on the menu was great and so were the prices.\n",
      "   CLEANED:  the selection on the menu was great and so were the prices\n"
     ]
    }
   ],
   "source": [
    "# Display first 5 reviews before and after cleaning\n",
    "print(\"Before vs After Cleaning:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"\\n{i+1}. ORIGINAL: {df['Review'].iloc[i]}\")\n",
    "    print(f\"   CLEANED:  {df['Review_Clean'].iloc[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06434cd0",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Remove Stopwords\n",
    "\n",
    "Remove common words that don't carry sentiment (the, and, is, etc.)  \n",
    "**Important:** We keep negation words (not, never, no) as they're critical for sentiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67c944ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stopword removal function created!\n",
      "üìä Total stopwords: 188\n",
      "‚ö†Ô∏è Kept 16 negation words for sentiment\n"
     ]
    }
   ],
   "source": [
    "# Get English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# CRITICAL: Keep negation words - they're important for sentiment!\n",
    "negation_words = {'not', 'no', 'nor', 'never', 'neither', 'nobody', \n",
    "                  'nothing', 'nowhere', \"don't\", \"didn't\", \"doesn't\", \n",
    "                  \"won't\", \"wouldn't\", \"shouldn't\", \"couldn't\", \"can't\"}\n",
    "\n",
    "# Remove negation words from stopwords\n",
    "stop_words = stop_words - negation_words\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"Remove common English stopwords from text (keeping negations)\"\"\"\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "print(f\"‚úÖ Stopword removal function created!\")\n",
    "print(f\"üìä Total stopwords: {len(stop_words)}\")\n",
    "print(f\"‚ö†Ô∏è Kept {len(negation_words)} negation words for sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c22b5abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stopwords removed from all reviews!\n",
      "\n",
      "Dataset now has 7 columns:\n",
      "['Review', 'Liked', 'review_length', 'word_count', 'Review_Clean', 'Review_Processed', 'processed_word_count']\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords from cleaned reviews\n",
    "df['Review_Processed'] = df['Review_Clean'].apply(remove_stopwords)\n",
    "\n",
    "print(\"‚úÖ Stopwords removed from all reviews!\")\n",
    "print(f\"\\nDataset now has {len(df.columns)} columns:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e691fb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification that 'not' is preserved:\n",
      "============================================================\n",
      "ORIGINAL:  Crust is not good.\n",
      "CLEANED:   crust is not good\n",
      "PROCESSED: crust not good\n",
      "\n",
      "‚úÖ 'not' successfully preserved!\n"
     ]
    }
   ],
   "source": [
    "# Verify \"not\" is preserved\n",
    "test_idx = 1  # \"Crust is not good\"\n",
    "print(\"Verification that 'not' is preserved:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ORIGINAL:  {df['Review'].iloc[test_idx]}\")\n",
    "print(f\"CLEANED:   {df['Review_Clean'].iloc[test_idx]}\")\n",
    "print(f\"PROCESSED: {df['Review_Processed'].iloc[test_idx]}\")\n",
    "print(\"\\n‚úÖ 'not' successfully preserved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca94e4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete Text Processing Pipeline:\n",
      "================================================================================\n",
      "\n",
      "1. ORIGINAL:  Wow... Loved this place.\n",
      "   CLEANED:   wow loved this place\n",
      "   PROCESSED: wow loved place\n",
      "\n",
      "2. ORIGINAL:  Crust is not good.\n",
      "   CLEANED:   crust is not good\n",
      "   PROCESSED: crust not good\n",
      "\n",
      "3. ORIGINAL:  Not tasty and the texture was just nasty.\n",
      "   CLEANED:   not tasty and the texture was just nasty\n",
      "   PROCESSED: not tasty texture nasty\n",
      "\n",
      "4. ORIGINAL:  Stopped by during the late May bank holiday off Rick Steve recommendation and loved it.\n",
      "   CLEANED:   stopped by during the late may bank holiday off rick steve recommendation and loved it\n",
      "   PROCESSED: stopped late may bank holiday rick steve recommendation loved\n",
      "\n",
      "5. ORIGINAL:  The selection on the menu was great and so were the prices.\n",
      "   CLEANED:   the selection on the menu was great and so were the prices\n",
      "   PROCESSED: selection menu great prices\n"
     ]
    }
   ],
   "source": [
    "# Show the complete processing pipeline\n",
    "print(\"Complete Text Processing Pipeline:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"\\n{i+1}. ORIGINAL:  {df['Review'].iloc[i]}\")\n",
    "    print(f\"   CLEANED:   {df['Review_Clean'].iloc[i]}\")\n",
    "    print(f\"   PROCESSED: {df['Review_Processed'].iloc[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85a3bdbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Count Comparison:\n",
      "============================================================\n",
      "Average words BEFORE stopword removal: 10.9\n",
      "Average words AFTER stopword removal:  5.8\n",
      "\n",
      "‚úÖ Reduced by ~5.1 words per review\n"
     ]
    }
   ],
   "source": [
    "# Calculate how many words were removed\n",
    "df['processed_word_count'] = df['Review_Processed'].apply(lambda x: len(x.split()))\n",
    "\n",
    "print(\"Word Count Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Average words BEFORE stopword removal: {df['word_count'].mean():.1f}\")\n",
    "print(f\"Average words AFTER stopword removal:  {df['processed_word_count'].mean():.1f}\")\n",
    "print(f\"\\n‚úÖ Reduced by ~{df['word_count'].mean() - df['processed_word_count'].mean():.1f} words per review\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028c6310",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ TF-IDF Vectorization\n",
    "\n",
    "Convert text to numerical features using Term Frequency-Inverse Document Frequency.\n",
    "\n",
    "**What is TF-IDF?**\n",
    "- Measures how important a word is to a review\n",
    "- Common words get lower scores\n",
    "- Rare but meaningful words get higher scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b419d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TF-IDF Vectorizer imported!\n"
     ]
    }
   ],
   "source": [
    "# Import TF-IDF vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "print(\"‚úÖ TF-IDF Vectorizer imported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed675231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Text vectorized successfully!\n",
      "üìä Shape of feature matrix: (996, 1000)\n",
      "   - 996 reviews\n",
      "   - 1000 features (words)\n"
     ]
    }
   ],
   "source": [
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=1000)  # Keep top 1000 words\n",
    "\n",
    "# Fit and transform the processed reviews\n",
    "X = vectorizer.fit_transform(df['Review_Processed'])\n",
    "\n",
    "# Extract target variable\n",
    "y = df['Liked'].values\n",
    "\n",
    "print(\"‚úÖ Text vectorized successfully!\")\n",
    "print(f\"üìä Shape of feature matrix: {X.shape}\")\n",
    "print(f\"   - {X.shape[0]} reviews\")\n",
    "print(f\"   - {X.shape[1]} features (words)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b94fc65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary: 1000 words\n",
      "\n",
      "First 20 features:\n",
      "['absolutely' 'acknowledged' 'actually' 'added' 'ago' 'ala' 'albondigas'\n",
      " 'allergy' 'almonds' 'almost' 'alone' 'also' 'although' 'always' 'amazing'\n",
      " 'amazingrge' 'ambiance' 'ambience' 'amount' 'ample']\n",
      "\n",
      "Last 20 features:\n",
      "['wont' 'word' 'work' 'world' 'worse' 'worst' 'worth' 'would' 'wouldnt'\n",
      " 'wow' 'wrap' 'wrong' 'year' 'years' 'yet' 'youd' 'youre' 'yum' 'yummy'\n",
      " 'zero']\n"
     ]
    }
   ],
   "source": [
    "# Get the feature names (words used by vectorizer)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"Total vocabulary: {len(feature_names)} words\")\n",
    "print(f\"\\nFirst 20 features:\")\n",
    "print(feature_names[:20])\n",
    "print(f\"\\nLast 20 features:\")\n",
    "print(feature_names[-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3aa2af",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Train-Test Split\n",
    "\n",
    "Split data into training (80%) and testing (20%) sets for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1455f4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Train-test split function imported!\n"
     ]
    }
   ],
   "source": [
    "# Import train-test split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"‚úÖ Train-test split function imported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07c0fc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data split successfully!\n",
      "\n",
      "Training set: 796 reviews (79.9%)\n",
      "Testing set:  200 reviews (20.1%)\n",
      "\n",
      "Target distribution in training set:\n",
      "  Positive: 399 (50.1%)\n",
      "  Negative: 397 (49.9%)\n"
     ]
    }
   ],
   "source": [
    "# Split data: 80% train, 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y  # Keep same ratio of positive/negative in both sets\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Data split successfully!\")\n",
    "print(f\"\\nTraining set: {X_train.shape[0]} reviews ({X_train.shape[0]/len(df)*100:.1f}%)\")\n",
    "print(f\"Testing set:  {X_test.shape[0]} reviews ({X_test.shape[0]/len(df)*100:.1f}%)\")\n",
    "print(f\"\\nTarget distribution in training set:\")\n",
    "print(f\"  Positive: {y_train.sum()} ({y_train.sum()/len(y_train)*100:.1f}%)\")\n",
    "print(f\"  Negative: {len(y_train) - y_train.sum()} ({(len(y_train) - y_train.sum())/len(y_train)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c691c342",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Save Processed Data\n",
    "\n",
    "Save the vectorized data and preprocessing objects for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "475d72f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All processed data saved successfully!\n",
      "\n",
      "üìÅ Saved files:\n",
      "   1. models/tfidf_vectorizer.pkl\n",
      "   2. data/processed/train_test_data.pkl\n"
     ]
    }
   ],
   "source": [
    "# Import pickle to save Python objects\n",
    "import pickle\n",
    "\n",
    "# Save the vectorizer (we'll need it for new predictions)\n",
    "with open('./models/tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "\n",
    "# Save the processed data\n",
    "with open('./data/processed/train_test_data.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'feature_names': feature_names\n",
    "    }, f)\n",
    "\n",
    "print(\"‚úÖ All processed data saved successfully!\")\n",
    "print(f\"\\nüìÅ Saved files:\")\n",
    "print(f\"   1. models/tfidf_vectorizer.pkl\")\n",
    "print(f\"   2. data/processed/train_test_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020e9abe",
   "metadata": {},
   "source": [
    "## üìä Phase 2 Complete!\n",
    "\n",
    "### Summary of Preprocessing Steps:\n",
    "\n",
    "1. ‚úÖ **Text Cleaning:** Converted to lowercase, removed punctuation\n",
    "2. ‚úÖ **Stopword Removal:** Removed common words (kept negations!)\n",
    "3. ‚úÖ **TF-IDF Vectorization:** Converted text to 1000 numerical features\n",
    "4. ‚úÖ **Train-Test Split:** 80% train (796 reviews), 20% test (200 reviews)\n",
    "5. ‚úÖ **Data Saved:** Ready for model training in Phase 3!\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps (Phase 3):\n",
    "- Train multiple classification models\n",
    "- Compare model performance\n",
    "- Evaluate accuracy, precision, recall\n",
    "- Select best model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
